{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"MemoryMesh <p>The SQLite of AI Memory. Persistent, intelligent memory for any LLM in three lines of code.</p> MIT License 100% Free Zero Dependencies Fully Local Any LLM pip install memorymesh Get Started GitHub Instant Blazing fast storage Persistent Memory across sessions Zero Dependencies to install 3 Lines To get started Three Lines. Persistent Memory. <pre><code>from memorymesh import MemoryMesh\n\nmemory = MemoryMesh()\nmemory.remember(\"User prefers Python and dark mode\")\nresults = memory.recall(\"What does the user prefer?\")\n</code></pre> <p>No servers. No cloud. No API keys. Everything stays on your machine in SQLite.</p> Works With Everything  Claude Code  Claude Desktop  GPT-4  Gemini CLI  Cursor  Windsurf  Ollama  Codex CLI  Llama  Mistral  GitHub Copilot  Any MCP Client Claude CodeGemini CLICodex CLICursorPython <pre><code>{ \"mcpServers\": { \"memorymesh\": { \"command\": \"memorymesh-mcp\" } } }\n</code></pre> <pre><code>{ \"mcpServers\": { \"memorymesh\": { \"command\": \"memorymesh-mcp\" } } }\n</code></pre> <pre><code>{ \"mcpServers\": { \"memorymesh\": { \"command\": \"memorymesh-mcp\" } } }\n</code></pre> <pre><code>{ \"mcpServers\": { \"memorymesh\": { \"command\": \"memorymesh-mcp\" } } }\n</code></pre> <pre><code>from memorymesh import MemoryMesh\nmemory = MemoryMesh(embedding=\"ollama\")\nmemory.remember(\"User prefers dark mode\")\n</code></pre> Why MemoryMesh? <p>AI tools have no memory. Existing solutions are heavy. MemoryMesh is different.</p> Sessions Start Blank <p>You told Claude your coding style yesterday. Today it asks again.</p> Decisions Are Lost <p>You decided JWT for auth. Next session, the AI suggests cookies.</p> Mistakes Repeat <p>Fixed a bug Monday. Friday, the AI introduces the same bug.</p> Tools Are Siloed <p>What you teach Claude stays in Claude. Gemini and Cursor know nothing.</p> SolutionApproachTrade-off Mem0SaaSCloud required, data leaves machine, ongoing costs LettaAgent frameworkHeavy lock-in, complex setup ZepMemory serverRequires PostgreSQL + Docker MemoryMeshEmbeddable libraryZero deps. SQLite. Free forever. What You Get Works with Any LLM <p>Claude, GPT, Gemini, Llama, Ollama, Mistral. A memory layer, not a framework.</p> Fully Local &amp; Private <p>All data in SQLite on your machine. No telemetry, no cloud, no data collection.</p> Cross-Tool Memory <p>Sync to Claude Code, Codex CLI, and Gemini CLI. One store, every tool.</p> Free &amp; Open Source <p>MIT licensed. No paid tiers, no subscriptions. Built for humanity.</p> Features Simple API <p>remember(), recall(), forget(). No boilerplate.</p> Semantic Search <p>Find by meaning, not keywords. Ollama or local embeddings.</p> Memory Categories <p>Auto-categorize and route to the right scope.</p> MCP Server <p>Built-in for Claude, Cursor, Windsurf, Gemini.</p> Encrypted Storage <p>Encrypt at rest. Zero external deps.</p> Auto-Compaction <p>Transparent dedup. Like SQLite auto-vacuum.</p> Session Context <p>User profile, guardrails, project state in one call.</p> Time-Based Decay <p>Recent memories rank higher. Stale ones fade.</p> Episodic Memory <p>Group by session. Recall with conversation context.</p> Pin Support <p>Pin critical memories. Never decay, always top-ranked.</p> Privacy Guard <p>Auto-detect and redact secrets before storing.</p> Contradiction Detection <p>Catch conflicting facts. Keep, update, or skip.</p> Retrieval Filters <p>Filter by category, importance, time range, metadata.</p> Web Dashboard <p>Browse and search memories in your browser.</p> Evaluation Suite <p>Quality and adversarial tests for recall accuracy.</p> What You Actually Get <p>MemoryMesh isn't about cramming more into the context window. It's about making every session smarter from the start.</p> Without MemoryMeshWith MemoryMesh Every session starts blankPreferences, decisions, and patterns load automatically Repeat yourself across toolsOne memory store shared across Claude, Gemini, Codex, Cursor AI re-discovers settled decisionsPast decisions recalled by semantic search Same bugs reintroducedPast mistakes remembered and avoided Context locked in one toolPortable memory that follows you Community <p>MemoryMesh is open source and built in the open. Get involved.</p> \u2b50 Star on GitHub <p>Show support and get release updates</p> \ud83d\udca1 Request a Feature <p>Tell us what you need</p> \ud83d\udc1b Report a Bug <p>Help us fix it</p> \u2709\ufe0f Subscribe for Updates \ud83d\udcac Discussions \ud83e\udd1d Contribute Give Your AI a Memory pip install memorymesh Get Started API Reference GitHub"},{"location":"api/","title":"API Reference","text":"<p>Full Python API for the <code>MemoryMesh</code> class.</p>"},{"location":"api/#core-methods","title":"Core Methods","text":"Method Description <code>remember(text, metadata, importance, decay_rate, scope, auto_importance, session_id, category, auto_categorize, pin, redact, on_conflict)</code> Store a new memory <code>recall(query, k, min_relevance, scope, session_id, category, min_importance, time_range, metadata_filter)</code> Recall top-k relevant memories <code>forget(memory_id)</code> Delete a specific memory (checks both stores) <code>forget_all(scope)</code> Delete all memories in a scope (default: <code>\"project\"</code>) <code>search(text, k)</code> Alias for <code>recall()</code> <code>get(memory_id)</code> Retrieve a memory by ID (checks both stores) <code>list(limit, offset, scope)</code> List memories with pagination <code>count(scope)</code> Get number of memories (scope: <code>None</code> for total) <code>get_time_range(scope)</code> Get oldest/newest timestamps <code>close()</code> Close both database connections"},{"location":"api/#episodic-memory-methods","title":"Episodic Memory Methods","text":"Method Description <code>get_session(session_id)</code> Retrieve all memories for a conversation session <code>list_sessions()</code> List all sessions with counts and timestamps"},{"location":"api/#session-context-methods","title":"Session &amp; Context Methods","text":"Method Description <code>session_start(project_context)</code> Retrieve structured context for a new AI session"},{"location":"api/#compaction-methods","title":"Compaction Methods","text":"Method Description <code>compact(scope, similarity_threshold, dry_run)</code> Detect and merge similar memories"},{"location":"api/#update-methods","title":"Update Methods","text":"Method Description <code>update(memory_id, text, importance, decay_rate, metadata, scope)</code> Update an existing memory in-place. Supports scope migration."},{"location":"api/#review-methods","title":"Review Methods","text":"Method Description <code>review_memories(mesh, scope, detectors, project_name)</code> Audit memories for quality issues (returns ReviewResult)"},{"location":"api/#constructor","title":"Constructor","text":"<pre><code>MemoryMesh(\n    path=None,                    # Project database path (None = global-only)\n    global_path=None,             # Global database path (default: ~/.memorymesh/global.db)\n    embedding=\"local\",            # \"none\", \"local\", \"ollama\", \"openai\"\n    encryption_key=None,          # Passphrase for at-rest encryption (optional)\n    relevance_weights=None,       # RelevanceWeights instance (optional)\n    **kwargs,                     # Embedding provider options\n)\n</code></pre>"},{"location":"api/#remember","title":"remember()","text":"<pre><code>memory.remember(\n    text=\"User prefers dark mode\",   # Required: the content to store\n    metadata={\"source\": \"chat\"},     # Optional: key-value metadata\n    importance=0.5,                  # Optional: importance score 0.0-1.0\n    decay_rate=0.01,                 # Optional: how fast importance fades\n    scope=None,                      # Optional: \"project\", \"global\", or None (auto-infer)\n    auto_importance=False,           # Optional: auto-score importance from text\n    session_id=None,                 # Optional: group into a conversation session\n    category=None,                   # Optional: \"preference\", \"guardrail\", \"mistake\", etc.\n    auto_categorize=False,           # Optional: auto-detect category from text\n    pin=False,                       # Optional: pin memory (importance=1.0, never decays)\n    redact=False,                    # Optional: redact detected secrets before storing\n    on_conflict=\"keep_both\",         # Optional: \"keep_both\", \"update\", or \"skip\"\n)\n</code></pre> <p>When <code>auto_importance=True</code>, the <code>importance</code> parameter is ignored and MemoryMesh scores it automatically based on text analysis.</p> <p>When <code>category</code> is set, the <code>scope</code> is automatically determined (e.g. <code>\"preference\"</code> -&gt; global, <code>\"decision\"</code> -&gt; project). When <code>auto_categorize=True</code>, category is detected from text heuristics and <code>auto_importance</code> is also enabled.</p> <p>When <code>pin=True</code>, importance is set to <code>1.0</code> and decay rate to <code>0.0</code>, ensuring the memory never fades.</p> <p>When <code>redact=True</code>, detected secrets (API keys, tokens, passwords) are replaced with <code>[REDACTED]</code> before storage.</p> <p>The <code>on_conflict</code> parameter controls contradiction handling: <code>\"keep_both\"</code> (default) stores both and flags the contradiction, <code>\"update\"</code> replaces the most similar existing memory, <code>\"skip\"</code> discards the new memory if a contradiction is found (returns empty string).</p> <p>When <code>scope</code> is <code>None</code> (default), MemoryMesh automatically infers the scope from the text content. Text about the user (preferences, habits, workflow) routes to global; text about the project (file paths, versions, implementations) routes to project. Set <code>scope</code> explicitly to override.</p> <p>Valid categories:</p> Category Scope Description <code>preference</code> global User coding style, tool preferences <code>guardrail</code> global Rules AI must follow <code>mistake</code> global Past mistakes to avoid <code>personality</code> global User character traits <code>question</code> global Recurring questions/concerns <code>decision</code> project Architecture/design decisions <code>pattern</code> project Code patterns and conventions <code>context</code> project Project-specific facts <code>session_summary</code> project Auto-generated session summaries"},{"location":"api/#update","title":"update()","text":"<pre><code>memory.update(\n    memory_id=\"abc123\",              # Required: ID of memory to update\n    text=\"Updated text\",             # Optional: new text (re-embeds if changed)\n    importance=0.8,                  # Optional: new importance score\n    decay_rate=0.0,                  # Optional: new decay rate\n    metadata={\"category\": \"decision\"},  # Optional: new metadata\n    scope=\"global\",                  # Optional: migrate to different scope\n)\n</code></pre> <p>When <code>scope</code> is provided, the memory is moved from its current store to the new scope's store (cross-scope migration). Only the fields you pass are changed -- omitted fields retain their current values.</p>"},{"location":"api/#recall","title":"recall()","text":"<pre><code>results = memory.recall(\n    query=\"What theme?\",             # Required: natural-language query\n    k=5,                             # Optional: max results to return\n    min_relevance=0.0,               # Optional: minimum relevance threshold\n    scope=None,                      # Optional: \"project\", \"global\", or None (both)\n    session_id=None,                 # Optional: boost memories from this session\n    category=None,                   # Optional: filter by category (e.g. \"decision\")\n    min_importance=None,             # Optional: minimum importance threshold\n    time_range=None,                 # Optional: (start_iso, end_iso) filter\n    metadata_filter=None,            # Optional: dict of key-value pairs to match\n)\n</code></pre> <p>When <code>session_id</code> is provided, memories from the same session receive a relevance boost in ranking.</p> <p>When <code>category</code>, <code>min_importance</code>, <code>time_range</code>, or <code>metadata_filter</code> are set, the candidate set is pre-filtered before ranking. This is more efficient than post-filtering, especially for large memory stores.</p>"},{"location":"api/#privacy-guard","title":"Privacy Guard","text":"<pre><code>from memorymesh.privacy import check_for_secrets, redact_secrets\n\n# Check for potential secrets\nsecrets = check_for_secrets(\"API key: sk-abc123456789\")\n# [\"API key\"]\n\n# Redact secrets\nclean = redact_secrets(\"token: sk-abc123456789\")\n# \"token: [REDACTED]\"\n</code></pre>"},{"location":"api/#contradiction-detection","title":"Contradiction Detection","text":"<pre><code>from memorymesh.contradiction import find_contradictions, ConflictMode\n\n# Find memories that may contradict new text\ncontradictions = find_contradictions(text, embedding, store, threshold=0.75)\n# Returns: [(memory, similarity_score), ...]\n\n# ConflictMode enum\nConflictMode.KEEP_BOTH   # Store both, flag contradiction\nConflictMode.UPDATE       # Replace most similar existing memory\nConflictMode.SKIP         # Discard new memory if contradiction found\n</code></pre>"},{"location":"api/#compact","title":"compact()","text":"<pre><code>result = memory.compact(\n    scope=\"project\",                 # Optional: scope to compact\n    similarity_threshold=0.85,       # Optional: Jaccard similarity threshold\n    dry_run=False,                   # Optional: preview without deleting\n)\n\nprint(result.merged_count)           # Number of merges performed\nprint(result.deleted_ids)            # IDs of memories that were merged away\nprint(result.kept_ids)               # IDs of memories that absorbed merges\n</code></pre>"},{"location":"api/#session_start","title":"session_start()","text":"<pre><code>context = memory.session_start(\n    project_context=\"working on auth module\",  # Optional: helps find relevant project memories\n)\n\n# Returns a structured dict:\n# {\n#     \"user_profile\": [\"Senior Python developer\", \"Prefers dark mode\"],\n#     \"guardrails\": [\"Never auto-commit without asking\"],\n#     \"common_mistakes\": [\"Forgot to run tests before pushing\"],\n#     \"common_questions\": [\"Always asks about test coverage\"],\n#     \"project_context\": [\"Uses SQLite for storage\", \"Google-style docstrings\"],\n#     \"last_session\": [\"Implemented auth module, 15 tests added\"],\n# }\n</code></pre>"},{"location":"api/#context-manager","title":"Context Manager","text":"<pre><code>with MemoryMesh() as memory:\n    memory.remember(\"User prefers TypeScript\")\n    results = memory.recall(\"programming language\")\n# Database connection is cleanly closed\n</code></pre>"},{"location":"api/#episodic-memory","title":"Episodic Memory","text":"<pre><code># Store memories with a session ID\nmemory.remember(\"User asked about auth\", session_id=\"session-001\")\nmemory.remember(\"Decided to use JWT\", session_id=\"session-001\")\n\n# Retrieve all memories from a session\nsession_memories = memory.get_session(\"session-001\")\n\n# List all sessions\nsessions = memory.list_sessions()\n# [{\"session_id\": \"session-001\", \"count\": 2, \"first_at\": \"...\", \"last_at\": \"...\"}]\n\n# Boost same-session memories during recall\nresults = memory.recall(\"authentication\", session_id=\"session-001\")\n</code></pre> <p>Back to Home</p>"},{"location":"architecture/","title":"Architecture","text":"<p>System design overview for MemoryMesh.</p> <pre><code>+-----------------------------------------------------+\n|                   Your Application                   |\n+-----------------------------------------------------+\n                          |\n                          v\n+-----------------------------------------------------+\n|               MemoryMesh Core (core.py)              |\n|   remember()     recall()     forget()     compact() |\n+-----------------------------------------------------+\n     |              |              |              |\n     v              v              v              v\n+-----------+ +-----------+ +-----------+ +-----------+\n|  Memory   | | Embedding | | Relevance | |Compaction |\n|  Store    | | Provider  | |  Engine   | |  Engine   |\n| store.py  | |embeddings | |relevance  | |compaction |\n|           | |  .py      | |  .py      | |  .py      |\n+-----------+ +-----------+ +-----------+ +-----------+\n     |              |\n     v              v\n+-----------+ +-----------+\n|Encryption | |  Auto-    |\n| (optional)| | Importance|\n|encryption | |auto_import|\n|  .py      | | ance.py   |\n+-----------+ +-----------+\n     |\n     v\n+-----------------------------------------------------+\n|                 SQLite Databases                      |\n|   ~/.memorymesh/global.db  (user-wide preferences)  |\n|   &lt;project&gt;/.memorymesh/memories.db  (per-project)  |\n+-----------------------------------------------------+\n</code></pre>"},{"location":"architecture/#understanding-scopes","title":"Understanding Scopes","text":"<pre><code>~/.memorymesh/global.db          &lt;project&gt;/.memorymesh/memories.db\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    GLOBAL STORE      \u2502          \u2502   PROJECT STORE      \u2502\n\u2502                      \u2502          \u2502                      \u2502\n\u2502  preferences         \u2502          \u2502  decisions           \u2502\n\u2502  guardrails          \u2502          \u2502  patterns            \u2502\n\u2502  mistakes            \u2502          \u2502  context             \u2502\n\u2502  personality         \u2502          \u2502  session summaries   \u2502\n\u2502  questions           \u2502          \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 recall() \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  searches both\n</code></pre> <ul> <li>Global store (<code>~/.memorymesh/global.db</code>) -- user-wide preferences, guardrails, mistakes, personality, and recurring questions. Shared across all projects.</li> <li>Project store (<code>&lt;project&gt;/.memorymesh/memories.db</code>) -- architecture decisions, code patterns, project context, and session summaries. Isolated per project.</li> </ul> <p><code>recall()</code> merges results from both stores by default. <code>forget_all()</code> clears only the project store unless you explicitly pass <code>scope=\"global\"</code>.</p>"},{"location":"architecture/#memory-lifecycle","title":"Memory Lifecycle","text":"<ol> <li>remember() -- Text is embedded (if an embedding provider is configured), optionally scanned for secrets, checked for contradictions, and stored in the appropriate SQLite database.</li> <li>recall() -- The query is embedded, then both vector similarity search and keyword fallback are used to find candidates. Results are ranked by a composite score of semantic similarity, recency, importance, and access frequency.</li> <li>forget() -- Deletes a specific memory by ID from whichever store contains it.</li> <li>Time decay -- Each memory has an importance score and a decay rate. Over time, <code>new_importance = importance * exp(-decay_rate * days_since_update)</code>. Pinned memories have <code>decay_rate=0</code> and never fade. Frequently accessed memories stay relevant because recall updates the <code>updated_at</code> timestamp.</li> </ol>"},{"location":"architecture/#key-modules","title":"Key Modules","text":"Module Purpose <code>core.py</code> Public API: <code>remember()</code>, <code>recall()</code>, <code>forget()</code>, <code>compact()</code>, session methods <code>store.py</code> SQLite storage layer with per-thread connections <code>embeddings.py</code> Pluggable embedding providers (local, ollama, openai, none) <code>relevance.py</code> Scoring engine: semantic similarity + recency + importance + frequency <code>compaction.py</code> Detect and merge similar/redundant memories <code>auto_importance.py</code> Heuristic-based importance scoring from text analysis <code>encryption.py</code> Application-level at-rest encryption for text and metadata <code>memory.py</code> <code>Memory</code> dataclass with session_id, serialization helpers <code>migrations.py</code> Schema versioning via <code>PRAGMA user_version</code>"},{"location":"architecture/#schema-migrations","title":"Schema Migrations","text":"<p>MemoryMesh automatically manages database schema upgrades. When you upgrade to a new version, existing databases are migrated in-place without data loss the next time they are opened.</p> <ul> <li>Fresh installs get the latest schema directly.</li> <li>Existing databases are detected and upgraded incrementally.</li> <li>Both project and global stores migrate independently.</li> <li>Migrations are additive-only -- no columns or data are ever deleted.</li> </ul>"},{"location":"architecture/#migration-history","title":"Migration History","text":"Version Description v1 Initial schema (memories table, importance and updated_at indexes) v2 Add <code>session_id</code> column and index for episodic memory <p>Schema versions are tracked using SQLite's built-in <code>PRAGMA user_version</code>. You can check the current version programmatically:</p> <pre><code>from memorymesh.store import MemoryStore\n\nstore = MemoryStore(path=\".memorymesh/memories.db\")\nprint(store.schema_version)  # e.g. 2\n</code></pre> <p>No manual steps are needed. Just upgrade the package and MemoryMesh handles the rest.</p>"},{"location":"architecture/#memory-data-model","title":"Memory Data Model","text":"<p>Each memory stores the following fields:</p> Field Type Description <code>id</code> TEXT Unique identifier (UUID hex) <code>text</code> TEXT The memory content (free-form text) <code>metadata_json</code> TEXT JSON key-value metadata <code>embedding_blob</code> BLOB Vector embedding (binary packed float32) <code>session_id</code> TEXT Optional conversation session identifier <code>created_at</code> TEXT ISO-8601 creation timestamp <code>updated_at</code> TEXT ISO-8601 last-access timestamp <code>access_count</code> INTEGER Number of times recalled <code>importance</code> REAL Importance score 0.0-1.0 <code>decay_rate</code> REAL Rate of importance decay over time <p>When encryption is enabled, <code>text</code> and <code>metadata_json</code> are encrypted before storage.</p> <p>Back to Home</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Performance benchmarks for MemoryMesh core operations.</p>"},{"location":"benchmarks/#results","title":"Results","text":"<p>On a typical development machine (keyword-only mode, <code>embedding=\"none\"</code>):</p> Operation Latency Notes <code>remember()</code> ~50 us/op Constant time, regardless of store size <code>recall()</code> at 100 memories ~120 us Top-5 results <code>recall()</code> at 1,000 memories ~350 us Top-5 results <code>recall()</code> at 5,000 memories &lt; 700 us Top-5 results <code>forget()</code> ~30 us/op Single memory deletion <code>list()</code> pagination ~200 us Page through 1,000 memories Concurrent throughput ~3,800 ops/s 4 threads, mixed read/write <code>auto_importance</code> scoring ~15 us/op Heuristic text analysis"},{"location":"benchmarks/#disk-usage","title":"Disk Usage","text":"Memory Count Database Size 100 ~20 KB 1,000 ~180 KB 5,000 ~900 KB 10,000 ~1.8 MB <p>SQLite with WAL mode keeps databases compact. Embeddings (when enabled) add ~3 KB per memory for 768-dimensional vectors.</p>"},{"location":"benchmarks/#running-benchmarks","title":"Running Benchmarks","text":"<pre><code># Via Makefile\nmake bench\n\n# Directly\npython -m benchmarks.bench_memorymesh\n\n# Or from the project root\n.venv/bin/python -m benchmarks.bench_memorymesh\n</code></pre>"},{"location":"benchmarks/#what-is-measured","title":"What Is Measured","text":"Benchmark Description remember() throughput Time to store N memories (10, 100, 1000, 5000) recall() latency Time to recall top-k from stores of various sizes forget() latency Time to delete memories from stores of various sizes list() pagination Time to paginate through a 1000-memory store concurrent access Multi-threaded remember/recall (4 threads, 50 ops each) store size on disk SQLite database file size at various memory counts auto_importance scoring Throughput of heuristic importance scoring compaction impact Before/after compaction performance and memory count episodic memory Session-based remember, get_session, list_sessions"},{"location":"benchmarks/#configuration","title":"Configuration","text":"<p>All benchmarks use <code>embedding=\"none\"</code> (keyword-only mode) by default for consistent, reproducible results without external dependencies. Each benchmark creates temporary databases that are cleaned up after the run.</p> <p>Results are printed to stdout as a formatted table and saved as JSON to <code>benchmarks/results.json</code> for tracking over time.</p>"},{"location":"benchmarks/#interpreting-results","title":"Interpreting Results","text":"<ul> <li>per-op times under 1ms are excellent for interactive use</li> <li>concurrent throughput above 500 ops/s indicates SQLite WAL mode is performing well</li> <li>store size should scale roughly linearly with memory count</li> <li>compaction should reduce both memory count and subsequent recall latency</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<p>The <code>memorymesh</code> CLI lets you inspect, manage, and sync your memory stores from the terminal.</p> Command Description <code>memorymesh list</code> List stored memories (table or JSON) <code>memorymesh search &lt;query&gt;</code> Search memories by keyword <code>memorymesh show &lt;id&gt;</code> Show full detail for a memory (supports partial IDs) <code>memorymesh stats</code> Show memory count, oldest/newest timestamps <code>memorymesh export</code> Export memories to JSON or HTML <code>memorymesh compact</code> Detect and merge similar/redundant memories <code>memorymesh init</code> Set up MemoryMesh for a project (MCP config + tool configs) <code>memorymesh sync</code> Sync memories to/from AI tool markdown files <code>memorymesh formats</code> List known format adapters and install status <code>memorymesh report</code> Generate a memory analytics report <code>memorymesh review</code> Audit memories for quality issues <p>Most commands accept <code>--scope project|global|all</code> to filter by store. Run <code>memorymesh &lt;command&gt; --help</code> for full options.</p>"},{"location":"cli/#memorymesh-ui","title":"<code>memorymesh ui</code>","text":"<p>Launch a web-based dashboard for viewing and managing memories.</p> <pre><code>memorymesh ui [--port PORT] [--no-open]\n</code></pre> <p>Options:</p> Option Default Description <code>--port PORT</code> <code>8765</code> Port to run the web server on <code>--no-open</code> Do not automatically open the browser <p>The dashboard provides a searchable, filterable view of all memories across both project and global stores. It runs entirely locally -- no data leaves your machine.</p>"},{"location":"cli/#memorymesh-review","title":"<code>memorymesh review</code>","text":"<p>Audit memories for quality issues and get an overall health score.</p> <pre><code>memorymesh review [--scope project|global|all] [--fix] [--verbose]\n</code></pre> <p>Options:</p> Option Default Description <code>--scope SCOPE</code> <code>all</code> Which scope to audit <code>--fix</code> Auto-fix what it can (add categories to uncategorized memories) <code>--verbose</code> Show each issue with memory preview and suggestion <p>The review system detects 6 types of issues:</p> Issue Severity Description <code>scope_mismatch</code> High Memory in wrong scope (e.g. product name in global) <code>too_verbose</code> Medium Text exceeds length limits (200 chars global, 500 project) <code>near_duplicate</code> Medium Similar to another memory (&gt;70% text similarity) <code>uncategorized</code> Low Missing category metadata <code>stale</code> Low Not accessed in 30+ days with low importance <code>low_quality</code> Low Low auto-importance score (&lt;0.4) <p>Quality score formula: <code>100 - (high * 10 + medium * 5 + low * 2)</code>, clamped to 0-100.</p>"},{"location":"cli/#compaction","title":"Compaction","text":"<p>The <code>compact</code> command detects and merges similar or redundant memories:</p> <pre><code># Preview what would be merged (dry run)\nmemorymesh compact --dry-run\n\n# Merge duplicates in the project store\nmemorymesh compact --scope project\n\n# Set a custom similarity threshold (default: 0.85)\nmemorymesh compact --threshold 0.9\n\n# Compact the global store\nmemorymesh compact --scope global\n</code></pre> <p>Compaction uses Jaccard word-set similarity to find near-duplicate memories. When two memories are similar above the threshold, they are merged: the higher-importance memory is kept, access counts are summed, and metadata is combined.</p> <p>Back to Home</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Everything you need to configure MemoryMesh: embedding providers, storage paths, encryption, and relevance tuning.</p>"},{"location":"configuration/#embedding-providers","title":"Embedding Providers","text":"<p>MemoryMesh supports multiple embedding backends. Choose the one that fits your constraints:</p> Provider Install Requires Best For <code>none</code> <code>pip install memorymesh</code> Nothing Getting started, keyword-based matching <code>local</code> (default) <code>pip install memorymesh[local]</code> ~500MB model download Privacy-sensitive apps, offline use <code>ollama</code> <code>pip install memorymesh[ollama]</code> Running Ollama instance Local semantic search, GPU acceleration <code>openai</code> <code>pip install memorymesh[openai]</code> OpenAI API key Highest quality embeddings <pre><code># Use local embeddings (runs on your machine, no API calls) -- this is the default\nmemory = MemoryMesh(embedding=\"local\")\n\n# Use Ollama (connect to local Ollama server)\nmemory = MemoryMesh(embedding=\"ollama\", ollama_model=\"nomic-embed-text\")\n\n# Use OpenAI embeddings\nmemory = MemoryMesh(embedding=\"openai\", openai_api_key=\"sk-...\")\n\n# No embeddings (pure keyword matching, zero dependencies)\nmemory = MemoryMesh(embedding=\"none\")\n</code></pre>"},{"location":"configuration/#using-ollama","title":"Using Ollama","text":""},{"location":"configuration/#what-is-ollama","title":"What is Ollama?","text":"<p>Ollama is a free, open-source application that runs AI models locally on your machine. Think of it like a local server -- it runs in the background and applications connect to it over HTTP. MemoryMesh uses Ollama for one specific purpose: converting text into numerical vectors (embeddings) that enable semantic search.</p> <p>Without Ollama (or another embedding provider), MemoryMesh falls back to keyword matching -- <code>recall(\"testing\")</code> will only find memories containing the exact word \"testing\". With Ollama, MemoryMesh understands meaning -- <code>recall(\"testing\")</code> finds memories about \"pytest\", \"unit tests\", \"test coverage\", and \"CI pipeline\" because they are semantically related.</p>"},{"location":"configuration/#how-it-works","title":"How it works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         HTTP (localhost:11434)        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Your AI Tool   \u2502                                       \u2502                  \u2502\n\u2502  (Claude Code,   \u2502                                       \u2502     Ollama       \u2502\n\u2502   Gemini CLI,    \u2502                                       \u2502  (background     \u2502\n\u2502   Cursor, etc.)  \u2502                                       \u2502   service)       \u2502\n\u2502        \u2502         \u2502                                       \u2502                  \u2502\n\u2502   MemoryMesh     \u2502  \u2500\u2500\u2500\u2500\u2500 \"embed this text\" \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;  \u2502  nomic-embed-    \u2502\n\u2502   (MCP server)   \u2502  &lt;\u2500\u2500\u2500\u2500 [0.02, -0.15, 0.89, ...] \u2500\u2500\u2500  \u2502  text model      \u2502\n\u2502                  \u2502         768 numbers back               \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    SQLite DB\n  (memories.db)\n</code></pre> <ol> <li>When you call <code>remember(\"User prefers dark mode\")</code>, MemoryMesh sends the text to Ollama</li> <li>Ollama runs the embedding model and returns a vector of 768 numbers representing the meaning</li> <li>MemoryMesh stores the text + vector in SQLite</li> <li>When you call <code>recall(\"theme preferences\")</code>, MemoryMesh embeds the query and finds stored memories with similar vectors</li> <li>This is why <code>recall(\"theme preferences\")</code> finds \"User prefers dark mode\" even though no words match</li> </ol>"},{"location":"configuration/#step-1-install-ollama","title":"Step 1: Install Ollama","text":"<p>Ollama is a separate application. Install it first:</p> <p>macOS: <pre><code>brew install ollama\n</code></pre></p> <p>Linux: <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre></p> <p>Windows: Download from ollama.com/download.</p>"},{"location":"configuration/#step-2-start-ollama","title":"Step 2: Start Ollama","text":"<p>Ollama runs as a background service on port 11434. You start it once and it stays running.</p> <pre><code># Start Ollama (runs in the background)\nollama serve\n</code></pre> <p>Already running? If you see \"address already in use\", Ollama is already running. This is fine -- on macOS with Homebrew it often auto-starts.</p> <pre><code># Check if Ollama is running\nbrew services info ollama       # macOS (Homebrew)\ncurl http://localhost:11434     # Any OS -- should return \"Ollama is running\"\n</code></pre>"},{"location":"configuration/#step-3-pull-the-embedding-model","title":"Step 3: Pull the embedding model","text":"<p>Download the embedding model that MemoryMesh will use. This is a one-time download (~274MB):</p> <pre><code>ollama pull nomic-embed-text\n</code></pre> <p>What is <code>nomic-embed-text</code>? It is an embedding model, not a chat model. It does one thing: convert text into a vector of 768 numbers that capture semantic meaning. Similar texts produce similar vectors. This is what powers MemoryMesh's semantic search.</p> Embedding Model Dimensions Size Quality Speed <code>nomic-embed-text</code> 768 274MB Very good Fast <code>all-minilm</code> 384 46MB Good Very fast <code>mxbai-embed-large</code> 1024 670MB Best Slower <p><code>nomic-embed-text</code> is the recommended default -- good quality, reasonable size, fast. You can use a different model by passing <code>ollama_model=\"model-name\"</code>.</p>"},{"location":"configuration/#step-4-install-and-configure-memorymesh","title":"Step 4: Install and configure MemoryMesh","text":"<pre><code># Install MemoryMesh (Ollama support uses only stdlib -- no extra deps needed)\npip install memorymesh\n</code></pre> <p>Important: You do NOT need a special install for Ollama support. <code>pip install memorymesh</code> is sufficient because MemoryMesh communicates with Ollama via HTTP using Python's built-in <code>urllib</code> -- no extra packages required.</p> <p>As a Python library: <pre><code>from memorymesh import MemoryMesh\n\nmemory = MemoryMesh(\n    embedding=\"ollama\",\n    ollama_model=\"nomic-embed-text\",           # default\n    # ollama_base_url=\"http://localhost:11434\", # default, change if Ollama is on another machine\n)\n</code></pre></p> <p>As an MCP server (for Claude Code, Gemini CLI, Cursor, etc.): <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\",\n      \"env\": {\n        \"MEMORYMESH_EMBEDDING\": \"ollama\",\n        \"MEMORYMESH_OLLAMA_MODEL\": \"nomic-embed-text\"\n      }\n    }\n  }\n}\n</code></pre></p>"},{"location":"configuration/#step-5-verify-it-works","title":"Step 5: Verify it works","text":"<pre><code># Quick test\npython -c \"\nfrom memorymesh import MemoryMesh\nm = MemoryMesh(embedding='ollama')\nm.remember('User prefers Python and dark mode')\nresults = m.recall('programming language preferences')\nprint(results[0].text if results else 'No results')\nm.close()\n\"\n</code></pre> <p>If you see \"User prefers Python and dark mode\", semantic search is working.</p>"},{"location":"configuration/#faq","title":"FAQ","text":"<p>Q: Does Ollama need to be running in the same terminal as MemoryMesh? No. Ollama is a background service. Once started, it listens on port 11434. MemoryMesh connects to it via HTTP from any process. You can run <code>pip install</code>, <code>memorymesh-mcp</code>, and your AI tools in any terminal.</p> <p>Q: Does Ollama use my GPU? Yes, if available. Ollama automatically uses your GPU (CUDA on Linux/Windows, Metal on macOS) for faster inference. The embedding model is small enough that CPU is also fast (~10ms per embedding).</p> <p>Q: Can I use Ollama on a remote server? Yes. Set <code>ollama_base_url=\"http://your-server:11434\"</code> or <code>MEMORYMESH_OLLAMA_BASE_URL</code> env var. MemoryMesh will connect to the remote Ollama instance.</p> <p>Q: What if Ollama is not running when MemoryMesh starts? MemoryMesh gracefully falls back to keyword-only search. Your memories are still stored and recalled, just without semantic matching. Start Ollama and the next <code>recall()</code> will use embeddings automatically.</p>"},{"location":"configuration/#constructor-options","title":"Constructor Options","text":"<pre><code>from memorymesh import MemoryMesh\n\nmemory = MemoryMesh(\n    # Storage (dual-store)\n    path=\".memorymesh/memories.db\",       # Project-specific database (optional)\n    global_path=\"~/.memorymesh/global.db\", # User-wide global database\n\n    # Embeddings\n    embedding=\"local\",                    # \"none\", \"local\", \"ollama\", \"openai\"\n\n    # Embedding provider options (passed as **kwargs)\n    # ollama_model=\"nomic-embed-text\",    # Ollama model name\n    # ollama_base_url=\"http://localhost:11434\",\n    # openai_api_key=\"sk-...\",            # OpenAI API key\n    # local_model=\"all-MiniLM-L6-v2\",    # sentence-transformers model\n    # local_device=\"cpu\",                 # PyTorch device\n\n    # Encryption (optional)\n    # encryption_key=\"my-secret-key\",     # Encrypt text and metadata at rest\n\n    # Relevance tuning (optional)\n    # relevance_weights=RelevanceWeights(\n    #     semantic=0.5,\n    #     recency=0.2,\n    #     importance=0.2,\n    #     frequency=0.1,\n    # ),\n)\n</code></pre>"},{"location":"configuration/#encrypted-storage","title":"Encrypted Storage","text":"<p>MemoryMesh can encrypt memory text and metadata at rest. Pass an <code>encryption_key</code> to the constructor:</p> <pre><code>memory = MemoryMesh(\n    path=\".memorymesh/memories.db\",\n    encryption_key=\"my-secret-passphrase\",\n)\n\n# Memories are encrypted before writing to SQLite\nmemory.remember(\"Sensitive API key: sk-abc123\")\n\n# Decrypted transparently on recall\nresults = memory.recall(\"API key\")\n</code></pre> <p>How it works:</p> <ul> <li>A key is derived from your passphrase using PBKDF2-HMAC-SHA256.</li> <li>The <code>text</code> and <code>metadata</code> fields are encrypted before storage and decrypted on read.</li> <li>IDs, timestamps, importance, and embeddings are not encrypted (needed for queries and indexing).</li> <li>A random salt is stored in the database and reused across sessions.</li> <li>Uses only Python standard library (<code>hashlib</code>, <code>hmac</code>, <code>os</code>) -- zero external dependencies.</li> </ul> <p>This protects against casual inspection of the database file on disk. It is not a substitute for full-disk encryption for highly sensitive data.</p>"},{"location":"configuration/#auto-importance-scoring","title":"Auto-Importance Scoring","text":"<p>Instead of manually setting importance on every <code>remember()</code> call, let MemoryMesh score it automatically:</p> <pre><code># Manual importance (default behavior)\nmemory.remember(\"User prefers dark mode\", importance=0.7)\n\n# Auto-scored importance based on text analysis\nmemory.remember(\"Critical security decision: use JWT with RS256\", auto_importance=True)\n</code></pre> <p>The auto-scorer analyzes text using four heuristic signals:</p> Signal Weight What it detects Keywords 35% Decision words (\"critical\", \"always\", \"security\") boost; tentative words (\"maybe\", \"temporary\") reduce Specificity 30% File paths, version numbers, proper nouns, URLs indicate high-value information Structure 20% Code patterns (backticks, function names, imports) suggest technical decisions Length 15% Very short texts score lower; detailed texts score higher <p>The output is clamped to <code>[0.0, 1.0]</code> with a baseline of <code>0.5</code>.</p>"},{"location":"configuration/#memory-categories","title":"Memory Categories","text":"<p>MemoryMesh supports automatic memory categorization. When you set a category, the scope is automatically routed:</p> <pre><code># Category determines scope automatically\nmemory.remember(\"I prefer dark mode\", category=\"preference\")        # -&gt; global\nmemory.remember(\"Never auto-commit\", category=\"guardrail\")          # -&gt; global\nmemory.remember(\"Chose SQLite over Postgres\", category=\"decision\")  # -&gt; project\n\n# Or let MemoryMesh detect the category from text\nmemory.remember(\"I always use black for formatting\", auto_categorize=True)\n# Detected as \"preference\" -&gt; stored in global scope\n</code></pre> Category Auto-Scope Description <code>preference</code> global User coding style, tool preferences <code>guardrail</code> global Rules AI must follow <code>mistake</code> global Past mistakes to avoid <code>personality</code> global User character traits <code>question</code> global Recurring questions/concerns <code>decision</code> project Architecture/design decisions <code>pattern</code> project Code patterns and conventions <code>context</code> project Project-specific facts <code>session_summary</code> project Auto-generated session summaries <p>When <code>auto_categorize=True</code>, MemoryMesh also enables <code>auto_importance=True</code> automatically.</p>"},{"location":"configuration/#scope-inference","title":"Scope Inference","text":"<p>When <code>scope</code> is not explicitly set in <code>remember()</code>, MemoryMesh automatically infers the correct scope from the text content:</p> <ul> <li>User-focused text -&gt; global scope: \"User prefers dark mode\", \"Krishna's workflow: review then merge\", \"Coding style: functional over OOP\"</li> <li>Project-focused text -&gt; project scope: file paths (<code>src/</code>, <code>*.py</code>), config files (<code>pyproject.toml</code>), implementation state, version numbers, commit hashes</li> </ul> <p>The inference uses a scoring system -- when both user and project signals are present, the stronger signal wins. Product/project names detected from the project directory add extra weight toward project scope.</p> <p>Category routing still applies first. Inference refines it when the subject disagrees -- for example, a memory categorized as \"pattern\" (normally project) that says \"Krishna's patterns: tests CLI hands-on\" is about the user, so it routes to global.</p> <p>To override inference, set <code>scope</code> explicitly:</p> <pre><code>memory.remember(\"User prefers dark mode\", scope=\"project\")  # forced project despite user-focused text\n</code></pre>"},{"location":"configuration/#session-start","title":"Session Start","text":"<p>Retrieve structured context at the beginning of every AI session:</p> <pre><code>context = memory.session_start(project_context=\"working on auth module\")\n\n# Returns:\n# {\n#     \"user_profile\": [\"Senior Python developer\", \"Prefers dark mode\"],\n#     \"guardrails\": [\"Never auto-commit without asking\"],\n#     \"common_mistakes\": [\"Forgot to run tests before pushing\"],\n#     \"common_questions\": [\"Always asks about test coverage\"],\n#     \"project_context\": [\"Uses SQLite for storage\", \"Google-style docstrings\"],\n#     \"last_session\": [\"Implemented auth module, 15 tests added\"],\n# }\n</code></pre> <p>This is available as an MCP tool (<code>session_start</code>) that AI assistants can call at the beginning of every conversation.</p>"},{"location":"configuration/#auto-compaction","title":"Auto-Compaction","text":"<p>MemoryMesh automatically detects and merges duplicate memories during normal operation. Every 50 <code>remember()</code> calls, a lightweight compaction pass runs in the background. This is like SQLite's auto-vacuum -- you never need to think about it.</p> <pre><code># Adjust the interval (default: 50)\nmemory.compact_interval = 100   # compact every 100 writes\nmemory.compact_interval = 0     # disable auto-compaction\n\n# Manual compaction is still available\nresult = memory.compact(scope=\"project\", dry_run=True)\nprint(f\"Would merge {result.merged_count} duplicates\")\n</code></pre>"},{"location":"configuration/#pin-support","title":"Pin Support","text":"<p>Pin critical memories so they never fade and always appear in recall results:</p> <pre><code>memory.remember(\"NEVER auto-commit without asking the user\", pin=True)\n</code></pre> <p>When <code>pin=True</code>:</p> <ul> <li>Importance is set to <code>1.0</code> (maximum).</li> <li>Decay rate is set to <code>0.0</code> (never fades).</li> <li>The metadata field <code>pinned: true</code> is set for identification.</li> </ul> <p>Use pinning for guardrails, non-negotiable rules, and critical identity facts that should always influence the AI's behavior.</p>"},{"location":"configuration/#privacy-guard","title":"Privacy Guard","text":"<p>MemoryMesh scans all text for potential secrets before storing. Detected patterns include:</p> Pattern Example API keys <code>sk-abc123...</code>, <code>pk-xyz...</code> GitHub tokens <code>ghp_...</code>, <code>gho_...</code> Passwords <code>password: mySecret</code> Private keys <code>-----BEGIN PRIVATE KEY-----</code> JWT tokens <code>eyJhbG...</code> AWS access keys <code>AKIA...</code> Slack tokens <code>xoxb-...</code> <p>When secrets are detected, a warning is logged and metadata flags (<code>has_secrets_warning</code>, <code>detected_secret_types</code>) are added. To automatically redact secrets before storing:</p> <pre><code>memory.remember(\"API key is sk-abc123456789\", redact=True)\n# Stored text: \"API key is [REDACTED]\"\n</code></pre> <p>You can also use the functions directly:</p> <pre><code>from memorymesh.privacy import check_for_secrets, redact_secrets\n\nsecrets = check_for_secrets(\"my password: hunter2\")\n# [\"password\"]\n\nclean = redact_secrets(\"token: sk-abc123456789\")\n# \"token: [REDACTED]\"\n</code></pre>"},{"location":"configuration/#contradiction-detection","title":"Contradiction Detection","text":"<p>When storing a new memory, MemoryMesh can check for existing memories that contradict it. Control the behavior with the <code>on_conflict</code> parameter:</p> <pre><code># Default: store both, flag the contradiction in metadata\nmemory.remember(\"Use PostgreSQL for production\", on_conflict=\"keep_both\")\n\n# Replace the most similar existing memory\nmemory.remember(\"Use PostgreSQL for production\", on_conflict=\"update\")\n\n# Don't store if a contradiction is found\nmemory.remember(\"Use PostgreSQL for production\", on_conflict=\"skip\")\n</code></pre> <p>The three conflict modes:</p> Mode Behavior <code>keep_both</code> Store the new memory alongside existing ones. Adds <code>has_contradiction</code> flag to metadata. <code>update</code> Replace the most similar existing memory with the new text. <code>skip</code> Discard the new memory if a contradiction is found. Returns empty string. <p>You can also find contradictions directly:</p> <pre><code>from memorymesh.contradiction import find_contradictions, ConflictMode\n\n# Find memories that may contradict new text\ncontradictions = find_contradictions(text, embedding, store, threshold=0.75)\n# Returns: [(memory, similarity_score), ...]\n</code></pre>"},{"location":"configuration/#retrieval-filters","title":"Retrieval Filters","text":"<p><code>recall()</code> supports additional filters to narrow down results:</p> <pre><code>results = memory.recall(\n    \"auth decisions\",\n    k=10,\n    category=\"decision\",           # Only return memories with this category\n    min_importance=0.7,            # Only return memories with importance &gt;= 0.7\n    time_range=(\"2026-01-01\", \"2026-02-01\"),  # Filter by creation date\n    metadata_filter={\"pinned\": True},         # Match specific metadata keys\n)\n</code></pre> Filter Type Description <code>category</code> <code>str</code> Only return memories with this category in metadata <code>min_importance</code> <code>float</code> Minimum importance threshold <code>time_range</code> <code>tuple[str, str]</code> ISO-8601 date range <code>(start, end)</code> for creation time <code>metadata_filter</code> <code>dict</code> Key-value pairs that must match in memory metadata <p>Filters are applied before ranking, so they reduce the candidate set rather than post-filtering results.</p> <p>Back to Home</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to MemoryMesh. This is an open project built to serve humanity by making AI memory accessible, free, and simple. Every contribution -- whether it is a bug fix, a feature, documentation, or a thoughtful issue report -- helps move that mission forward.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or later</li> <li>Git</li> </ul>"},{"location":"contributing/#setting-up-the-development-environment","title":"Setting Up the Development Environment","text":"<ol> <li> <p>Fork and clone the repository:</p> <pre><code>git clone https://github.com/YOUR_USERNAME/memorymesh.git\ncd memorymesh\n</code></pre> </li> <li> <p>Create a virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre> </li> <li> <p>Install in development mode with all dependencies:</p> <pre><code>pip install -e \".[dev,all]\"\n</code></pre> </li> <li> <p>Verify everything works:</p> <pre><code>make all   # Runs lint + tests + type checking\n</code></pre> </li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use ruff for both linting and formatting. The configuration is in <code>pyproject.toml</code>.</p>"},{"location":"contributing/#requirements","title":"Requirements","text":"<ul> <li>Type hints are required on all public functions and methods.</li> <li>Docstrings are required on all public classes, methods, and functions. Use Google-style docstrings.</li> <li>No wildcard imports. Always import specific names.</li> <li>Prefer dataclasses over plain dictionaries for structured data.</li> </ul>"},{"location":"contributing/#running-the-linter-and-formatter","title":"Running the Linter and Formatter","text":"<pre><code>make lint      # Check for issues\nmake format    # Auto-fix formatting\nmake typecheck # Run mypy\n</code></pre> <p>Please ensure <code>make lint</code> and <code>make typecheck</code> pass before submitting a pull request.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>We use pytest for all testing.</p> <pre><code>make test              # Run the full test suite\npytest tests/ -x       # Stop on first failure\npytest tests/ -v       # Verbose output\npytest tests/ -k name  # Run tests matching a pattern\n</code></pre>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":"<ul> <li>All new features must include tests.</li> <li>All bug fixes must include a regression test.</li> <li>Tests should be fast. Avoid network calls in unit tests; mock external services.</li> <li>Place tests in the <code>tests/</code> directory, mirroring the source structure.</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Fork the repository and create a new branch from <code>main</code>:</p> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> </li> <li> <p>Make your changes. Keep commits focused and atomic.</p> </li> <li> <p>Run the full check suite:</p> <pre><code>make all\n</code></pre> </li> <li> <p>Push your branch and open a pull request against <code>main</code>.</p> </li> <li> <p>Fill out the PR template. Describe what changed, why, and how to test it.</p> </li> <li> <p>Respond to review feedback. We aim to review all PRs within a few days.</p> </li> </ol>"},{"location":"contributing/#pr-checklist","title":"PR Checklist","text":"<ul> <li> Code follows the project style guidelines</li> <li> Type hints are included for all public APIs</li> <li> Docstrings are included for all public APIs</li> <li> Tests are added or updated</li> <li> <code>make lint</code> passes</li> <li> <code>make typecheck</code> passes</li> <li> <code>make test</code> passes</li> <li> Documentation is updated if needed</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>We use GitHub Issues for tracking bugs and feature requests.</p>"},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Include:</p> <ul> <li>A clear description of the problem</li> <li>Steps to reproduce</li> <li>Expected vs. actual behavior</li> <li>Your environment (OS, Python version, MemoryMesh version, embedding provider)</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>Include:</p> <ul> <li>A description of the feature</li> <li>The use case it addresses</li> <li>A proposed solution (if you have one)</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project follows the Contributor Covenant Code of Conduct. By participating, you agree to uphold a welcoming, inclusive, and respectful environment for everyone.</p> <p>Thank you for helping build the future of open AI memory.</p>"},{"location":"faq/","title":"FAQ","text":"<p>Common questions about MemoryMesh.</p>"},{"location":"faq/#why-sqlite-not-plain-md-files","title":"Why SQLite, not plain .md files?","text":"<p>SQLite is the engine. Markdown is the view. This is the same pattern browsers use -- they store bookmarks in SQLite but display them as a list.</p> <p>Plain markdown files cannot do: vector similarity search, importance scoring, access counting, time-based decay, metadata filtering, or atomic transactions. MemoryMesh uses SQLite for all of that, and syncs a readable snapshot to <code>.md</code> files for tools that need them.</p>"},{"location":"faq/#why-not-a-full-rag-vector-database-pinecone-weaviate","title":"Why not a full RAG / vector database (Pinecone, Weaviate)?","text":"<p>MemoryMesh already IS local RAG. It embeds text, stores vectors, computes cosine similarity, and ranks results -- all in-process, all local. For AI memory scale (hundreds to low thousands of memories), SQLite with in-process similarity is faster and simpler than a separate database server. Zero infrastructure, zero cost, zero network latency.</p>"},{"location":"faq/#why-structured-storage-for-unstructured-data","title":"Why structured storage for unstructured data?","text":"<p>The text is unstructured -- <code>remember(\"whatever you want\")</code> accepts any free-form string. The metadata is structured: importance scores, timestamps, access counts, decay rates, embeddings. The structure is invisible plumbing that makes recall smart. You never see it unless you want to.</p>"},{"location":"faq/#what-does-semantic-search-mean","title":"What does \"semantic search\" mean?","text":"<p>Instead of matching exact keywords, semantic search understands meaning. Searching \"How do we handle auth?\" finds memories about authentication even if they never contain the word \"auth.\" This requires an embedding provider (local, Ollama, or OpenAI). Without one, MemoryMesh falls back to keyword matching, which still works well for most use cases.</p>"},{"location":"faq/#what-is-the-difference-between-standalone-and-with-ollama","title":"What is the difference between standalone and with Ollama?","text":"<p>Standalone (<code>embedding=\"none\"</code>) uses keyword matching -- fast, zero dependencies, good for most use cases. With Ollama (<code>embedding=\"ollama\"</code>) you get semantic search via a local model -- better recall accuracy, still fully local, no API keys. Ollama runs on your machine just like MemoryMesh.</p>"},{"location":"faq/#do-i-need-an-api-key","title":"Do I need an API key?","text":"<p>No. The base install works with zero dependencies and zero API keys. Ollama embeddings are also free and local. Only OpenAI embeddings require an API key.</p>"},{"location":"faq/#can-i-use-memorymesh-with-multiple-ai-tools-at-once","title":"Can I use MemoryMesh with multiple AI tools at once?","text":"<p>Yes. MemoryMesh stores memories in SQLite and can sync to Claude Code (<code>MEMORY.md</code>), Codex CLI (<code>AGENTS.md</code>), and Gemini CLI (<code>GEMINI.md</code>) simultaneously. Run <code>memorymesh sync --to auto --format all</code> and your knowledge follows you across tools.</p>"},{"location":"faq/#what-is-auto-importance-scoring","title":"What is auto-importance scoring?","text":"<p>When you call <code>remember(text, auto_importance=True)</code>, MemoryMesh analyzes the text and assigns an importance score automatically. It uses heuristics -- keyword detection (words like \"critical\", \"security\", \"decision\" boost importance), specificity (file paths, version numbers), structure (code patterns), and length. No ML models needed, pure Python, zero dependencies.</p>"},{"location":"faq/#what-is-episodic-memory","title":"What is episodic memory?","text":"<p>Episodic memory groups memories by conversation session. Pass a <code>session_id</code> to <code>remember()</code> and later use the same <code>session_id</code> in <code>recall()</code> to boost memories from the same conversation. Use <code>get_session()</code> to retrieve all memories from a session, and <code>list_sessions()</code> to see all sessions. This gives AI better continuity across multi-turn interactions.</p>"},{"location":"faq/#what-does-memory-compaction-do","title":"What does memory compaction do?","text":"<p>Over time, your memory store accumulates similar or redundant entries. <code>compact()</code> detects near-duplicate memories using Jaccard word-set similarity and merges them -- keeping the higher-importance version, combining metadata, and summing access counts. Run <code>memorymesh compact --dry-run</code> to preview what would be merged before committing.</p>"},{"location":"faq/#is-the-encryption-secure-enough-for-production","title":"Is the encryption secure enough for production?","text":"<p>The encrypted storage feature protects against casual inspection of the database file on disk. It uses PBKDF2-HMAC-SHA256 for key derivation and HMAC-authenticated encryption with zero external dependencies. For highly sensitive data, combine it with full-disk encryption (FileVault, BitLocker, LUKS). The encryption is not a substitute for proper secrets management.</p>"},{"location":"faq/#how-fast-is-memorymesh","title":"How fast is MemoryMesh?","text":"<p>On a typical machine: <code>remember()</code> runs at ~50us/op, <code>recall()</code> under 700us even at 5,000 memories (keyword mode), concurrent throughput hits ~3,800 ops/s with 4 threads. Run <code>make bench</code> to benchmark on your hardware.</p>"},{"location":"faq/#what-is-the-difference-between-project-and-global-scope","title":"What is the difference between project and global scope?","text":"<p>Global scope is your backpack -- it follows you to every project. It stores preferences, guardrails, mistakes, and personality traits. The database lives at <code>~/.memorymesh/global.db</code>.</p> <p>Project scope is your desk -- it stays in one project. It stores architecture decisions, code patterns, and project context. The database lives at <code>&lt;project&gt;/.memorymesh/memories.db</code>.</p> <p><code>recall()</code> searches both scopes by default. <code>forget_all()</code> only clears the project scope unless you explicitly pass <code>scope=\"global\"</code>.</p>"},{"location":"faq/#how-do-i-store-user-preferences","title":"How do I store user preferences?","text":"<p>Use the <code>\"preference\"</code> category, which auto-routes to global scope:</p> <pre><code>memory.remember(\"I prefer dark mode\", category=\"preference\")\nmemory.remember(\"Always use TypeScript over JavaScript\", category=\"preference\")\n</code></pre> <p>Or use <code>auto_categorize=True</code> and let MemoryMesh detect it from text:</p> <pre><code>memory.remember(\"I always use black for formatting\", auto_categorize=True)\n</code></pre>"},{"location":"faq/#what-are-guardrails","title":"What are guardrails?","text":"<p>Guardrails are rules the AI must follow. They are stored in global scope (available across all projects) and are surfaced at the start of every session via <code>session_start()</code>.</p> <pre><code>memory.remember(\"Never auto-commit without asking\", category=\"guardrail\")\nmemory.remember(\"Always run tests before suggesting code is complete\", category=\"guardrail\")\n</code></pre>"},{"location":"faq/#how-does-contradiction-detection-work","title":"How does contradiction detection work?","text":"<p>When you call <code>remember()</code>, MemoryMesh can check for existing memories that say something different. For example, if you have \"Use PostgreSQL for production\" and then store \"Use MySQL for production\", that is a contradiction.</p> <p>Control behavior with the <code>on_conflict</code> parameter:</p> <ul> <li><code>\"keep_both\"</code> (default) -- stores both memories and flags the contradiction in metadata.</li> <li><code>\"update\"</code> -- replaces the most similar existing memory with the new one.</li> <li><code>\"skip\"</code> -- silently discards the new memory if a contradiction is found.</li> </ul> <p>Contradiction detection uses embedding similarity (when available) or keyword overlap as a fallback.</p>"},{"location":"faq/#what-secrets-does-the-privacy-guard-detect","title":"What secrets does the privacy guard detect?","text":"<p>MemoryMesh scans for: API keys (<code>sk-...</code>, <code>pk-...</code>), GitHub tokens (<code>ghp_...</code>), passwords (<code>password: ...</code>), private keys (<code>-----BEGIN PRIVATE KEY-----</code>), JWT tokens (<code>eyJ...</code>), AWS access keys (<code>AKIA...</code>), and Slack tokens (<code>xoxb-...</code>).</p> <p>When secrets are detected, a warning is logged and metadata flags are set. Use <code>redact=True</code> on <code>remember()</code> to automatically replace secrets with <code>[REDACTED]</code> before storage.</p>"},{"location":"faq/#how-do-i-pin-important-memories","title":"How do I pin important memories?","text":"<p>Use <code>pin=True</code> on <code>remember()</code>:</p> <pre><code>memory.remember(\"NEVER deploy on Fridays\", pin=True)\n</code></pre> <p>Pinned memories have importance <code>1.0</code>, never decay, and always rank highly in recall results. Use pinning for critical guardrails and non-negotiable rules.</p> <p>Back to Home</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get MemoryMesh running in under 60 seconds.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"Base (zero dependencies)With Ollama (semantic search)With local embeddingsWith OpenAI embeddings <pre><code>pip install memorymesh\n</code></pre> <p>Uses keyword matching for recall. No external dependencies at all.</p> <pre><code>pip install memorymesh\nollama pull nomic-embed-text\n</code></pre> <p>Ollama support uses Python's built-in <code>urllib</code> -- no extra packages needed. See Ollama setup for details.</p> <pre><code>pip install \"memorymesh[local]\"\n</code></pre> <p>Uses <code>sentence-transformers</code> for fully offline semantic search. Downloads a ~500MB model on first use.</p> <pre><code>pip install \"memorymesh[openai]\"\n</code></pre> <p>Requires an OpenAI API key. Highest quality embeddings.</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#as-a-python-library","title":"As a Python Library","text":"<pre><code>from memorymesh import MemoryMesh\n\n# Create a memory instance (stores in SQLite, fully local)\nmemory = MemoryMesh(embedding=\"none\")  # or \"ollama\", \"local\", \"openai\"\n\n# Store memories\nmemory.remember(\"User is a senior Python developer\")\nmemory.remember(\"User prefers dark mode and concise explanations\")\nmemory.remember(\"Project uses SQLite for storage\")\n\n# Recall relevant memories\nresults = memory.recall(\"What does the user prefer?\")\nfor mem in results:\n    print(mem.text)\n\n# Clean up\nmemory.close()\n</code></pre>"},{"location":"getting-started/#as-an-mcp-server-for-ai-assistants","title":"As an MCP Server (for AI Assistants)","text":"<p>MemoryMesh includes a built-in MCP server that gives AI assistants persistent memory.</p> <p>1. Install MemoryMesh:</p> <pre><code>pip install memorymesh\n</code></pre> <p>2. Configure your AI tool:</p> Claude CodeClaude DesktopGemini CLIOpenAI Codex CLICursor / Windsurf <p>Add to <code>~/.claude/settings.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to <code>claude_desktop_config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to your Gemini CLI MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to your Codex CLI MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to <code>.cursor/mcp.json</code> or equivalent:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>3. Or use the auto-setup command:</p> <pre><code>memorymesh init\n</code></pre> <p>This auto-detects your installed AI tools and configures all of them.</p> <p>Recommended: Enable Semantic Search with Ollama</p> <p>By default, the MCP server uses keyword matching -- <code>recall(\"testing\")</code> only finds memories containing the exact word \"testing\". We strongly recommend adding Ollama for semantic search, which understands meaning -- <code>recall(\"testing\")</code> finds memories about \"pytest\", \"unit tests\", and \"CI pipeline\". Setup takes 2 minutes:</p> <pre><code>brew install ollama          # macOS (or curl install on Linux)\nollama pull nomic-embed-text # one-time ~274MB download\n</code></pre> <p>Then add <code>\"env\": { \"MEMORYMESH_EMBEDDING\": \"ollama\" }</code> to your MCP config. See full Ollama setup.</p>"},{"location":"getting-started/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/#understanding-scopes","title":"Understanding Scopes","text":"<p>MemoryMesh organizes memories into two scopes:</p> <ul> <li>Global = your backpack (follows you everywhere). Carries your preferences, guardrails, mistakes you have learned from, and personality traits. These apply no matter what project you are working on.</li> <li>Project = your desk (stays in one project). Holds architecture decisions, code patterns, project-specific context, and session summaries. These are relevant only to the current project.</li> </ul> <p>When you call <code>recall()</code>, both scopes are searched by default and the results are merged. When you call <code>forget_all()</code>, only the project scope is cleared -- your global memories are protected.</p> <p>Best practices for what to remember:</p> Store in global scope Store in project scope Do not store Coding style preferences Architecture decisions Trivial one-time facts Rules you always follow Code patterns and conventions Temporary state Your identity and traits Project-specific context Verbatim code snippets Recurring mistakes Session summaries Anything already in CLAUDE.md"},{"location":"getting-started/#dual-store-architecture","title":"Dual-Store Architecture","text":"<p>MemoryMesh uses two SQLite databases:</p> <pre><code>~/.memorymesh/\n  global.db                  # User preferences, identity, cross-project facts\n\n&lt;your-project&gt;/.memorymesh/\n  memories.db                # Project-specific decisions, patterns, context\n</code></pre> <ul> <li>Global store -- shared across all projects. User preferences, guardrails, personality.</li> <li>Project store -- isolated per project. Architecture decisions, code patterns, project context.</li> </ul> <p><code>recall()</code> searches both stores by default and merges results.</p>"},{"location":"getting-started/#memory-categories","title":"Memory Categories","text":"<p>MemoryMesh automatically routes memories to the correct store based on category:</p> <pre><code># These go to the global store automatically\nmemory.remember(\"I prefer dark mode\", category=\"preference\")\nmemory.remember(\"Never auto-commit\", category=\"guardrail\")\n\n# These stay in the project store\nmemory.remember(\"Chose JWT for auth\", category=\"decision\")\nmemory.remember(\"Uses Google-style docstrings\", category=\"pattern\")\n\n# Or let MemoryMesh detect the category from text\nmemory.remember(\"I always use black for formatting\", auto_categorize=True)\n</code></pre> Category Store What it captures <code>preference</code> Global Coding style, tool preferences <code>guardrail</code> Global Rules the AI must follow <code>mistake</code> Global Past errors to avoid <code>personality</code> Global User traits and identity <code>question</code> Global Recurring concerns <code>decision</code> Project Architecture and design choices <code>pattern</code> Project Code conventions <code>context</code> Project Project-specific facts <code>session_summary</code> Project Conversation summaries"},{"location":"getting-started/#relevance-scoring","title":"Relevance Scoring","text":"<p>When you call <code>recall()</code>, MemoryMesh ranks results using four signals:</p> Signal Weight Description Semantic similarity 50% How closely the query matches the memory's meaning Recency 20% More recent memories score higher Importance 20% Higher-importance memories score higher Frequency 10% Frequently accessed memories score higher <p>Memories also decay over time, just like human memory. Important, frequently-used memories persist; stale, low-importance ones fade naturally.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration -- Embedding providers, Ollama setup, encryption, tuning</li> <li>MCP Server -- Full MCP setup guide for AI assistants</li> <li>API Reference -- Complete Python API documentation</li> <li>CLI Reference -- Terminal commands for managing memories</li> </ul>"},{"location":"mcp-server/","title":"MCP Server","text":"<p>MemoryMesh includes a built-in MCP (Model Context Protocol) server that lets AI assistants use your memory directly as a tool. No API keys required for the default setup.</p>"},{"location":"mcp-server/#quick-setup","title":"Quick Setup","text":"<p>The fastest way to configure everything:</p> <pre><code>memorymesh init\n</code></pre> <p>This auto-detects your installed AI tools and configures all of them. Or set up manually below.</p>"},{"location":"mcp-server/#setup-by-tool","title":"Setup by Tool","text":"Claude CodeClaude DesktopGemini CLIOpenAI Codex CLICursorWindsurfGitHub CopilotOther MCP Clients <p>Add to <code>~/.claude/settings.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to <code>claude_desktop_config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to your Gemini CLI MCP settings (<code>~/.gemini/settings.json</code> or project-level config):</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to your Codex CLI MCP settings (<code>~/.codex/config.json</code> or project-level config):</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to <code>.cursor/mcp.json</code> in your project root:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Add to your Windsurf MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>GitHub Copilot reads <code>AGENTS.md</code>, <code>CLAUDE.md</code>, and <code>GEMINI.md</code> files. Use MemoryMesh sync to keep these files updated:</p> <pre><code>memorymesh sync --format all\n</code></pre> <p>Copilot doesn't support MCP directly, but it benefits from the synced memory files.</p> <p>Any tool that supports the MCP protocol can connect to MemoryMesh. The MCP server uses stdin/stdout JSON-RPC:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\"\n    }\n  }\n}\n</code></pre> <p>Recommended: Enable Semantic Search with Ollama</p> <p>By default, the MCP server uses keyword matching. For significantly better recall accuracy, set up Ollama for semantic search. With Ollama, <code>recall(\"testing\")</code> finds memories about \"pytest\", \"unit tests\", and \"CI pipeline\" -- not just exact word matches. See Enabling Semantic Search below.</p>"},{"location":"mcp-server/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>MEMORYMESH_PATH</code> Auto-detected Path to the project SQLite database <code>MEMORYMESH_GLOBAL_PATH</code> <code>~/.memorymesh/global.db</code> Path to the global SQLite database <code>MEMORYMESH_PROJECT_ROOT</code> Auto-detected Project root directory <code>MEMORYMESH_EMBEDDING</code> <code>none</code> Embedding provider (<code>none</code>, <code>local</code>, <code>ollama</code>, <code>openai</code>) <code>MEMORYMESH_OLLAMA_MODEL</code> <code>nomic-embed-text</code> Ollama model name <code>OPENAI_API_KEY</code> -- Required only when using <code>openai</code> embeddings"},{"location":"mcp-server/#enabling-semantic-search-ollama","title":"Enabling Semantic Search (Ollama)","text":"<p>By default, the MCP server uses keyword matching (<code>MEMORYMESH_EMBEDDING=none</code>). We strongly recommend enabling Ollama for semantic search -- it dramatically improves recall quality. With Ollama, searching for \"testing\" finds memories about \"pytest\", \"unit tests\", and \"CI pipeline\", not just exact keyword matches.</p> <p>Step 1: Install and start Ollama (one-time setup):</p> <pre><code># macOS\nbrew install ollama &amp;&amp; ollama pull nomic-embed-text\n\n# Linux\ncurl -fsSL https://ollama.com/install.sh | sh &amp;&amp; ollama pull nomic-embed-text\n</code></pre> <p>Step 2: Update your MCP config to enable Ollama embeddings:</p> Claude CodeClaude DesktopGemini CLICodex CLICursor / Windsurf <p>Update <code>~/.claude/settings.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\",\n      \"env\": {\n        \"MEMORYMESH_EMBEDDING\": \"ollama\",\n        \"MEMORYMESH_OLLAMA_MODEL\": \"nomic-embed-text\"\n      }\n    }\n  }\n}\n</code></pre> <p>Update <code>claude_desktop_config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\",\n      \"env\": {\n        \"MEMORYMESH_EMBEDDING\": \"ollama\",\n        \"MEMORYMESH_OLLAMA_MODEL\": \"nomic-embed-text\"\n      }\n    }\n  }\n}\n</code></pre> <p>Update your Gemini CLI MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\",\n      \"env\": {\n        \"MEMORYMESH_EMBEDDING\": \"ollama\",\n        \"MEMORYMESH_OLLAMA_MODEL\": \"nomic-embed-text\"\n      }\n    }\n  }\n}\n</code></pre> <p>Update your Codex CLI MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\",\n      \"env\": {\n        \"MEMORYMESH_EMBEDDING\": \"ollama\",\n        \"MEMORYMESH_OLLAMA_MODEL\": \"nomic-embed-text\"\n      }\n    }\n  }\n}\n</code></pre> <p>Update <code>.cursor/mcp.json</code> or equivalent:</p> <pre><code>{\n  \"mcpServers\": {\n    \"memorymesh\": {\n      \"command\": \"memorymesh-mcp\",\n      \"env\": {\n        \"MEMORYMESH_EMBEDDING\": \"ollama\",\n        \"MEMORYMESH_OLLAMA_MODEL\": \"nomic-embed-text\"\n      }\n    }\n  }\n}\n</code></pre> <p>Ollama runs locally -- no API keys, no cloud, no cost. See Configuration &gt; Using Ollama for full setup details and troubleshooting.</p>"},{"location":"mcp-server/#hybrid-memory-architecture","title":"Hybrid Memory Architecture","text":"<p>The MCP server uses a hybrid dual-store architecture that separates project-specific and global memories:</p> <pre><code>~/.memorymesh/\n  global.db                    &lt;- user preferences, identity, cross-project facts\n\n&lt;project-root&gt;/.memorymesh/\n  memories.db                  &lt;- project-specific memories, decisions, patterns\n</code></pre> <p>The project root is automatically detected from MCP client roots, the <code>MEMORYMESH_PROJECT_ROOT</code> environment variable, or the current working directory (if it contains <code>.git</code> or <code>pyproject.toml</code>).</p> <p>All tools accept an optional <code>scope</code> parameter (<code>\"project\"</code> or <code>\"global\"</code>): - <code>remember(scope=\"project\")</code> -- stores in the project database (default) - <code>remember(scope=\"global\")</code> -- stores in the user-wide database - <code>recall()</code> -- searches both databases by default - <code>forget_all(scope=\"project\")</code> -- only clears project memories (default; global is protected)</p>"},{"location":"mcp-server/#available-tools","title":"Available Tools","text":"<p>Once connected, your AI assistant gains these tools:</p> <ul> <li><code>remember</code> -- Store facts, preferences, and decisions. Supports <code>scope</code>, <code>category</code> (auto-routes scope), and <code>auto_categorize</code> (detect category from text).</li> <li><code>recall</code> -- Search memories by natural language query (supports <code>scope</code>)</li> <li><code>forget</code> -- Delete a specific memory by ID (searches both stores)</li> <li><code>forget_all</code> -- Delete all memories in a scope (defaults to project)</li> <li><code>memory_stats</code> -- View memory count and timestamps (supports <code>scope</code>)</li> <li><code>session_start</code> -- Retrieve structured context for the start of a new session. Returns user profile, guardrails, common mistakes, and project context. Call this at the beginning of every conversation.</li> <li><code>update_memory</code> -- Update an existing memory's text, importance, metadata, or scope. Supports cross-scope migration.</li> <li><code>review_memories</code> -- Audit memories for quality issues. Returns issues list with severity ratings and an overall quality score (0-100).</li> </ul> <p>No API keys are needed for the default setup. The MCP server uses keyword matching out of the box. Add an embedding provider for semantic search.</p>"},{"location":"mcp-server/#teaching-your-ai-to-use-memorymesh","title":"Teaching Your AI to Use MemoryMesh","text":"<p>Installing the MCP server gives your AI assistant the ability to use memory. But LLMs do not use tools proactively unless you tell them to. MemoryMesh works alongside your AI tool's existing memory -- it adds structured, searchable, cross-tool persistence on top of what's already there.</p> <p>The fastest way to set everything up is:</p> <pre><code>memorymesh init\n</code></pre> <p>This auto-detects which AI tools you have installed and adds MemoryMesh instructions to each one. You can also target a single tool:</p> <pre><code>memorymesh init --only claude\nmemorymesh init --only codex\nmemorymesh init --only gemini\n</code></pre> <p>Below is what each tool needs and the exact text to add if you prefer to do it manually.</p>"},{"location":"mcp-server/#claude-code","title":"Claude Code","text":"<p>Add a <code>## Memory (MemoryMesh)</code> section to your project's <code>CLAUDE.md</code>:</p> <pre><code>## Memory (MemoryMesh)\n\nMemoryMesh is configured as an MCP tool in this project. It adds persistent,\nstructured, cross-tool memory on top of your existing memory system. Use it\nalongside your default memory -- it enhances, not replaces.\n\n### At the start of every conversation\n\nCall `mcp__memorymesh__recall` with a summary of the user's request to load\nprior context, decisions, and patterns. If `session_start` is available,\ncall it to load user profile, guardrails, and project context.\n\n### When to `recall`\n\n- **Start of every conversation**: Check for relevant prior context.\n- **Before making decisions**: Check if this was decided before.\n- **When debugging**: Check if this problem was encountered previously.\n\n### When to `remember`\n\n- **When the user says \"remember this\"**: Store it with a category.\n- **After completing a task**: Store key decisions and patterns.\n  Use `category` to classify: `\"decision\"`, `\"pattern\"`, `\"context\"`.\n- **When the user teaches you something**: Use `category: \"preference\"`\n  or `category: \"guardrail\"` -- these auto-route to global scope.\n- **After fixing a non-trivial bug**: Use `category: \"mistake\"`.\n\n### Scope guidance\n\nCategories auto-route scope. If not using categories:\n- Use `scope: \"project\"` for project-specific decisions.\n- Use `scope: \"global\"` for user preferences and identity.\n</code></pre>"},{"location":"mcp-server/#openai-codex-cli","title":"OpenAI Codex CLI","text":"<p>Add a <code>## Memory (MemoryMesh)</code> section to your project's <code>AGENTS.md</code>:</p> <pre><code>## Memory (MemoryMesh)\n\nMemoryMesh adds persistent, structured memory on top of your existing system.\nIt enhances your default memory with semantic search, categories, and\ncross-tool sync.\n\n- At the start of every task, call `recall` with a summary to load prior\n  context. If `session_start` is available, call it for full user profile.\n- Call `recall` before making decisions to check for prior context.\n- After completing work, call `remember` with a `category` to store key\n  decisions (`\"decision\"`), patterns (`\"pattern\"`), or context (`\"context\"`).\n- When the user says \"remember this\", store it with `category: \"preference\"`\n  or `category: \"guardrail\"` for user-level facts.\n</code></pre>"},{"location":"mcp-server/#google-gemini-cli","title":"Google Gemini CLI","text":"<p>Add a <code>## Memory (MemoryMesh)</code> section to your project's <code>GEMINI.md</code>:</p> <pre><code>## Memory (MemoryMesh)\n\nMemoryMesh adds persistent, structured memory on top of your existing system.\nIt enhances your default memory with semantic search, categories, and\ncross-tool sync.\n\n- At the start of every task, call `recall` with a summary to load prior\n  context. If `session_start` is available, call it for full user profile.\n- Call `recall` before making decisions to check for prior context.\n- After completing work, call `remember` with a `category` to store key\n  decisions (`\"decision\"`), patterns (`\"pattern\"`), or context (`\"context\"`).\n- When the user says \"remember this\", store it with `category: \"preference\"`\n  or `category: \"guardrail\"` for user-level facts.\n</code></pre>"},{"location":"mcp-server/#generic-other-mcp-compatible-tools","title":"Generic / Other MCP-Compatible Tools","text":"<p>For any tool that supports MCP:</p> <ol> <li>Add the MCP server config (see Setup by Tool above).</li> <li>Add instructions to the tool's system prompt telling it to call <code>recall</code> at the start of conversations and <code>remember</code> after completing work. MemoryMesh works alongside existing memory -- no need to disable anything.</li> </ol> <p>Back to Home</p>"},{"location":"multi-tool-sync/","title":"Multi-Tool Memory Sync","text":"<p>MemoryMesh stores all memories in SQLite. The sync feature exports them to the markdown files that each AI tool reads natively, so your knowledge follows you across tools.</p> <pre><code># Export to Claude Code's MEMORY.md\nmemorymesh sync --to auto --format claude\n\n# Export to Codex CLI's AGENTS.md\nmemorymesh sync --to auto --format codex\n\n# Export to Gemini CLI's GEMINI.md\nmemorymesh sync --to auto --format gemini\n\n# Export to ALL detected tools at once\nmemorymesh sync --to auto --format all\n\n# Import memories FROM a markdown file back into MemoryMesh\nmemorymesh sync --from AGENTS.md --format codex\n\n# List which tools are detected on your system\nmemorymesh formats\n</code></pre>"},{"location":"multi-tool-sync/#how-it-works","title":"How it works","text":"<ul> <li>Each tool gets its own format adapter that outputs native markdown (no MemoryMesh-specific markup visible to the tool).</li> <li>Exports write only to a <code>## MemoryMesh Synced Memories</code> section, preserving any content you wrote yourself.</li> <li>Importance scores round-trip via invisible HTML comments, so re-importing preserves priority.</li> <li>Use <code>--to auto</code> to let MemoryMesh detect the correct file path for each tool.</li> </ul>"},{"location":"multi-tool-sync/#category-aware-sync","title":"Category-Aware Sync","text":"<p>When memories have categories (set via <code>category</code> parameter or <code>auto_categorize=True</code>), the sync output is automatically organized into structured sections:</p> <pre><code>## User Profile\n- [importance: 0.95] Senior Python developer, prefers concise explanations\n\n## Guardrails\n- [importance: 1.00] Never auto-commit without asking\n\n## Decisions\n- [importance: 0.90] Chose SQLite over PostgreSQL for simplicity\n\n## Project Context\n- [importance: 0.85] Main entry point is src/core.py\n</code></pre> <p>Memories without categories continue to use the existing topic/importance-tier grouping. The structured format makes MEMORY.md more useful as an always-on system prompt -- the AI can quickly find guardrails, understand the user, and recall project context.</p> <p>Back to Home</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#what-memorymesh-is","title":"What MemoryMesh Is","text":"<p>MemoryMesh is the SQLite of AI Memory -- an embeddable, zero-dependency Python library that gives any LLM application persistent, intelligent memory. It serves two audiences:</p> <ul> <li>AI tool users (Claude Code, Cursor, Gemini CLI) -- MemoryMesh runs as an invisible backend, powering <code>.md</code> memory files with ranked, structured data. You install it once and forget it exists.</li> <li>Developers building LLM apps -- MemoryMesh is an embeddable library. Three lines of Python give your agents long-term memory backed by SQLite. No servers, no infrastructure, no vendor lock-in.</li> </ul>"},{"location":"roadmap/#whats-next-v40-invisible-memory","title":"What's Next: v4.0 -- Invisible Memory","text":"<p>The AI shouldn't need to \"use\" MemoryMesh. It should just work.</p>"},{"location":"roadmap/#smart-sync","title":"Smart Sync","text":"<p>Export the top-N most relevant memories to <code>.md</code> files, ranked by importance and recency -- not a full dump. Directly reduces token cost by injecting only what matters into every session.</p>"},{"location":"roadmap/#auto-remember-hooks","title":"Auto-Remember Hooks","text":"<p>PostToolUse and Stop hooks that capture decisions, patterns, and key facts without requiring the AI to call <code>remember()</code>. Zero-instruction persistence -- memory happens as a side effect of working.</p>"},{"location":"roadmap/#lean-mcp","title":"Lean MCP","text":"<p>Consolidate the current 10 MCP tools into fewer, more powerful operations. Less schema overhead per session, lower cognitive load for the AI, same capabilities.</p>"},{"location":"roadmap/#task-aware-injection","title":"Task-Aware Injection","text":"<p><code>session_start</code> reads the user's first message and generates targeted context instead of a generic profile dump. The AI gets exactly the memories relevant to what it's about to do.</p>"},{"location":"roadmap/#measured-overhead","title":"Measured Overhead","text":"<p>Instrument real token impact per session. Track how many tokens MemoryMesh adds vs. saves. Self-optimize. Prove the value with numbers, not claims.</p>"},{"location":"roadmap/#v50-vision-anticipatory-intelligence","title":"v5.0 Vision -- Anticipatory Intelligence","text":"<ul> <li>Question learning -- Store questions users ask, proactively surface answers in future sessions</li> <li>Behavioral tracking -- Learn coding styles, interaction patterns, and preferred approaches across sessions</li> <li>Proactive anticipation -- Use accumulated data to anticipate needs before the user asks</li> <li>Multi-device sync -- Same memory available on every machine</li> <li>Cross-session continuity -- Understand narrative arcs that flat files cannot represent</li> </ul>"},{"location":"roadmap/#completed-milestones","title":"Completed Milestones","text":"Version Milestone v1.0 Production-ready core: episodic memory, auto-importance, encrypted storage, compaction v2.0 Personality engine: 9 memory categories, auto-categorization, session_start, structured sync v3.0 Intelligent memory: pin support, privacy guard, contradiction detection, retrieval filters, web dashboard v3.1 Setup &amp; diagnostics: improved onboarding, health checks, runtime reconfiguration"}]}